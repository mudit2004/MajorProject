{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mudit2004/MajorProject/blob/main/Main_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import necessary dependencies**"
      ],
      "metadata": {
        "id": "AZw4fI3TZFfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch_xla"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXHGqGOlZdvS",
        "outputId": "64dd5e97-7b0f-42f7-8fc2-949e9467294b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_xla\n",
            "  Downloading torch_xla-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from torch_xla) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_xla) (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from torch_xla) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_xla) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla) (2025.4.26)\n",
            "Downloading torch_xla-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (96.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_xla\n",
            "Successfully installed torch_xla-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pretrainedmodels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUm20L7JZoP7",
        "outputId": "0fd6888b-a930-4a87-d10f-480ac4343199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretrainedmodels\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (0.21.0+cu124)\n",
            "Collecting munch (from pretrainedmodels)\n",
            "  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pretrainedmodels) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->pretrainedmodels)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pretrainedmodels) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pretrainedmodels) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->pretrainedmodels) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pretrainedmodels) (3.0.2)\n",
            "Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretrainedmodels\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=15076a45d1e0bfe21662c1f3a8ff2ffd28c84bc2b587832cb0923426ef5ce1f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/5b/96/fd94bc35962d7c6b699e8814db545155ac91d2b95785e1b035\n",
            "Successfully built pretrainedmodels\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, munch, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pretrainedmodels\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed munch-4.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pretrainedmodels-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.24.4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "GpTDT0Z4DVBL",
        "outputId": "b094d21f-c289-4ea1-f3f9-13eeee2b7ac0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.3.1 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "6a8c1daee99643bb950ea29932a8e80b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations==0.5.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSyz8ovxahfs",
        "outputId": "45d8778e-ec2a-484c-fcbe-5a8c0449acfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==0.5.0\n",
            "  Downloading albumentations-0.5.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==0.5.0) (1.24.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from albumentations==0.5.0) (1.15.2)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==0.5.0) (0.25.2)\n",
            "Collecting imgaug>=0.4.0 (from albumentations==0.5.0)\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations==0.5.0) (6.0.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==0.5.0) (4.11.0.86)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->albumentations==0.5.0) (1.17.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->albumentations==0.5.0) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->albumentations==0.5.0) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->albumentations==0.5.0) (4.11.0.86)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->albumentations==0.5.0) (2.37.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->albumentations==0.5.0) (2.1.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.0) (3.4.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.0) (2025.3.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.0) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==0.5.0) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.5.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.5.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.5.0) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.5.0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.5.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.5.0) (2.9.0.post0)\n",
            "Downloading albumentations-0.5.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.5/70.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imgaug, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 2.0.6\n",
            "    Uninstalling albumentations-2.0.6:\n",
            "      Successfully uninstalled albumentations-2.0.6\n",
            "Successfully installed albumentations-0.5.0 imgaug-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch import Tensor\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataloader import _utils\n",
        "\n",
        "from random import choice\n",
        "\n",
        "from skimage import io\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "import glob\n",
        "\n",
        "#from torchsummary import summary\n",
        "import logging\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "# from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "# from apex import amp\n",
        "\n",
        "import random\n",
        "\n",
        "import time\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from albumentations.augmentations.transforms import Lambda, ShiftScaleRotate, HorizontalFlip, Normalize, RandomBrightnessContrast, RandomResizedCrop\n",
        "from albumentations.pytorch import ToTensor\n",
        "from albumentations import Compose, OneOrOther\n",
        "\n",
        "import warnings\n",
        "import torch_xla\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from pretrainedmodels import se_resnext101_32x4d, se_resnet152, xception, inceptionv4, inceptionresnetv2\n",
        "from torchvision.models import resnet34, resnet50"
      ],
      "metadata": {
        "id": "6Xw7hC18ZQA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_My_resnet34():\n",
        "    model = resnet34(pretrained = True)\n",
        "    output_channels = model.fc.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_resnet50():\n",
        "    model = resnet50(pretrained = True)\n",
        "    output_channels = model.fc.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_se_resnet152():\n",
        "    model = se_resnet152(pretrained = None)\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = nn.Sequential(*list(model.children())[:-2])\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_se_resnext101_32x4d():\n",
        "    model = se_resnext101_32x4d()\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = nn.Sequential(*list(model.children())[:-2])\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_inceptionv4():\n",
        "    model = inceptionv4()\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "\n",
        "    model = nn.Sequential(*model)\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_inceptionresnetv2():\n",
        "    model = inceptionresnetv2()\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "\n",
        "    model = nn.Sequential(*model)\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_resnet34():\n",
        "    model = resnet34(pretrained = True)\n",
        "    output_channels = model.fc.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_resnet50():\n",
        "    model = resnet50(pretrained = True)\n",
        "    output_channels = model.fc.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_se_resnet152():\n",
        "    model = se_resnet152(pretrained = None)\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = nn.Sequential(*list(model.children())[:-2])\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_se_resnext101_32x4d():\n",
        "    model = se_resnext101_32x4d()\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = nn.Sequential(*list(model.children())[:-2])\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_inceptionv4():\n",
        "    model = inceptionv4()\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "\n",
        "    model = nn.Sequential(*model)\n",
        "    return model, output_channels\n",
        "\n",
        "def get_My_inceptionresnetv2():\n",
        "    model = inceptionresnetv2()\n",
        "    output_channels = model.last_linear.in_features\n",
        "    model = list(model.children())[:-2]\n",
        "\n",
        "class Pooling_attention(nn.Module):\n",
        "  def __init__(self, input_channels, kernel_size = 1):\n",
        "    super(Pooling_attention, self).__init__()\n",
        "    self.pooling_attention = nn.Sequential(\n",
        "        nn.Conv2d(input_channels, 1, kernel_size = kernel_size, padding = kernel_size//2),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.pooling_attention(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Part_Relation(nn.Module):\n",
        "  def __init__(self, input_channels, reduction = [16], level = 1):\n",
        "    super(Part_Relation, self).__init__()\n",
        "\n",
        "    modules = []\n",
        "    for i in range(level):\n",
        "        output_channels = input_channels//reduction[i]\n",
        "        modules.append(nn.Conv2d(input_channels, output_channels, kernel_size = 1))\n",
        "        modules.append(nn.BatchNorm2d(output_channels))\n",
        "        modules.append(nn.ReLU())\n",
        "        input_channels = output_channels\n",
        "\n",
        "    self.pooling_attention_0 = nn.Sequential(*modules)\n",
        "    self.pooling_attention_1 = Pooling_attention(input_channels, 1)\n",
        "    self.pooling_attention_3 = Pooling_attention(input_channels, 3)\n",
        "    self.pooling_attention_5 = Pooling_attention(input_channels, 5)\n",
        "\n",
        "    self.last_conv =  nn.Sequential(\n",
        "        nn.Conv2d(3, 1, kernel_size = 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    input = x\n",
        "    x = self.pooling_attention_0(x)\n",
        "    x = torch.cat([self.pooling_attention_1(x), self.pooling_attention_3(x), self.pooling_attention_5(x)], dim = 1)\n",
        "    x = self.last_conv(x)\n",
        "    return input - input*x\n",
        "\n",
        "    model = nn.Sequential(*model)\n",
        "    return model, output_channels\n",
        "\n",
        "class BAA_New(nn.Module):\n",
        "    def __init__(self, gender_encode_length, backbone, out_channels):\n",
        "        super(BAA_New, self).__init__()\n",
        "        self.backbone0 = nn.Sequential(*backbone[0:5])\n",
        "        self.part_relation0 = Part_Relation(256)\n",
        "        self.out_channels = out_channels\n",
        "        self.backbone1 = backbone[5]\n",
        "        self.part_relation1 = Part_Relation(512, [4, 8], 2)\n",
        "        self.backbone2 = backbone[6]\n",
        "        self.part_relation2 = Part_Relation(1024, [8, 8], 2)\n",
        "        self.backbone3 = backbone[7]\n",
        "        self.part_relation3 = Part_Relation(2048, [8, 16], 2)\n",
        "\n",
        "        #3.788\n",
        "        # self.part_relation0 = Part_Relation(256)\n",
        "        # self.part_relation1 = Part_Relation(512, 32)\n",
        "        # self.part_relation2 = Part_Relation(1024, 8, 2)\n",
        "        # self.part_relation3 = Part_Relation(2048, 8, 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.gender_encoder = nn.Linear(1, gender_encode_length)\n",
        "        self.gender_bn = nn.BatchNorm1d(gender_encode_length)\n",
        "\n",
        "        self.fc0 = nn.Linear(out_channels + gender_encode_length, 1024)\n",
        "        self.bn0 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.output = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, image, gender):\n",
        "        x = self.part_relation0(self.backbone0(image))\n",
        "        # x  = self.backbone0(image)\n",
        "        x = self.part_relation1(self.backbone1(x))\n",
        "        # x = self.backbone1(x)\n",
        "        x = self.part_relation2(self.backbone2(x))\n",
        "        # x = self.backbone2(x)\n",
        "        x = self.part_relation3(self.backbone3(x))\n",
        "        # x = self.backbone3(x)\n",
        "        feature_map = x\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        x = torch.squeeze(x)\n",
        "        x = x.view(-1, self.out_channels)\n",
        "        image_feature = x\n",
        "\n",
        "        gender_encode = self.gender_bn(self.gender_encoder(gender))\n",
        "        gender_encode = F.relu(gender_encode)\n",
        "\n",
        "        x = torch.cat([x,  gender_encode], dim = 1)\n",
        "\n",
        "        x = F.relu(self.bn0(self.fc0(x)))\n",
        "\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "        x = self.output(x)\n",
        "\n",
        "        return feature_map, gender_encode, image_feature, x\n",
        "\n",
        "    def fine_tune(self, need_fine_tune = True):\n",
        "        self.train(need_fine_tune)\n",
        "\n",
        "\n",
        "\n",
        "class BAA_Base(nn.Module):\n",
        "    def __init__(self, gender_encode_length, backbone, out_channels):\n",
        "        super(BAA_New, self).__init__()\n",
        "        self.backbone0 = nn.Sequential(*backbone[0:5])\n",
        "        self.part_relation0 = Part_Relation(256)\n",
        "        self.out_channels = out_channels\n",
        "        self.backbone1 = backbone[5]\n",
        "        self.part_relation1 = Part_Relation(512, [4, 8], 2)\n",
        "        self.backbone2 = backbone[6]\n",
        "        self.part_relation2 = Part_Relation(1024, [8, 8], 2)\n",
        "        self.backbone3 = backbone[7]\n",
        "        self.part_relation3 = Part_Relation(2048, [8, 16], 2)\n",
        "\n",
        "        #3.788\n",
        "        # self.part_relation0 = Part_Relation(256)\n",
        "        # self.part_relation1 = Part_Relation(512, 32)\n",
        "        # self.part_relation2 = Part_Relation(1024, 8, 2)\n",
        "        # self.part_relation3 = Part_Relation(2048, 8, 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.gender_encoder = nn.Linear(1, gender_encode_length)\n",
        "        self.gender_bn = nn.BatchNorm1d(gender_encode_length)\n",
        "\n",
        "        self.fc0 = nn.Linear(out_channels + gender_encode_length, 1024)\n",
        "        self.bn0 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.output = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, image, gender):\n",
        "        x = self.part_relation0(self.backbone0(image))\n",
        "        # x  = self.backbone0(image)\n",
        "        x = self.part_relation1(self.backbone1(x))\n",
        "        # x = self.backbone1(x)\n",
        "        x = self.part_relation2(self.backbone2(x))\n",
        "        # x = self.backbone2(x)\n",
        "        x = self.part_relation3(self.backbone3(x))\n",
        "        # x = self.backbone3(x)\n",
        "        feature_map = x\n",
        "        x = F.adaptive_avg_pool2d(x, 1)\n",
        "        x = torch.squeeze(x)\n",
        "        x = x.view(-1, self.out_channels)\n",
        "        image_feature = x\n",
        "\n",
        "        gender_encode = self.gender_bn(self.gender_encoder(gender))\n",
        "        gender_encode = F.relu(gender_encode)\n",
        "\n",
        "        x = torch.cat([x,  gender_encode], dim = 1)\n",
        "\n",
        "        x = F.relu(self.bn0(self.fc0(x)))\n",
        "\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "        x = self.output(x)\n",
        "\n",
        "        return  x\n",
        "\n",
        "    def fine_tune(self, need_fine_tune = True):\n",
        "        self.train(need_fine_tune)\n",
        "\n",
        "\n",
        "\n",
        "class Self_Attention_Adj(nn.Module):\n",
        "    def __init__(self, feature_size, attention_size):\n",
        "        super(Self_Attention_Adj, self).__init__()\n",
        "        self.queue = nn.Parameter(torch.empty(feature_size, attention_size))\n",
        "        nn.init.kaiming_uniform_(self.queue)\n",
        "\n",
        "        self.key = nn.Parameter(torch.empty(feature_size, attention_size))\n",
        "        nn.init.kaiming_uniform_(self.key)\n",
        "\n",
        "        self.leak_relu = nn.LeakyReLU()\n",
        "\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)\n",
        "        Q = self.leak_relu(torch.matmul(x, self.queue))\n",
        "        K = self.leak_relu(torch.matmul(x, self.key))\n",
        "\n",
        "        return self.softmax(torch.matmul(Q, K.transpose(1, 2)))\n",
        "\n",
        "\n",
        "\n",
        "class Graph_GCN(nn.Module):\n",
        "    def __init__(self, node_size, feature_size, output_size):\n",
        "        super(Graph_GCN, self).__init__()\n",
        "        self.node_size = node_size\n",
        "        self.feature_size = feature_size\n",
        "        self.output_size = output_size\n",
        "        self.weight = nn.Parameter(torch.empty(feature_size, output_size))\n",
        "        nn.init.kaiming_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, x, A):\n",
        "        x = torch.matmul(A, x.transpose(1, 2))\n",
        "        return (torch.matmul(x, self.weight)).transpose(1, 2)\n",
        "\n",
        "\n",
        "class Graph_BAA(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super(Graph_BAA, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        #freeze image backbone\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.adj_learning = Self_Attention_Adj(2048, 256)\n",
        "        self.gconv = Graph_GCN(16*16, 2048, 1024)\n",
        "\n",
        "        self.fc0 = nn.Linear(1024 + 32, 1024)\n",
        "        self.bn0 = nn.BatchNorm1d(1024)\n",
        "\n",
        "        # self.fc1 = nn.Linear(1024, 512)\n",
        "        # self.bn1 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.output = nn.Linear(1024, 1)\n",
        "\n",
        "    def forward(self, image, gender):\n",
        "        #input image to backbone, 16*16*2048\n",
        "        feature_map, gender, image_feature, cnn_result = self.backbone(image, gender)\n",
        "        node_feature = feature_map.view(-1, 2048, 16*16)\n",
        "        A = self.adj_learning(node_feature)\n",
        "        x = F.leaky_relu(self.gconv(node_feature, A))\n",
        "        x = torch.squeeze(F.adaptive_avg_pool1d(x, 1))\n",
        "        graph_feature = x\n",
        "        x = torch.cat([x, gender], dim = 1)\n",
        "\n",
        "        x = F.relu(self.bn0(self.fc0(x)))\n",
        "        # x = F.relu(self.bn1(self.fc1(x)))\n",
        "\n",
        "        return image_feature,  graph_feature, gender, (self.output(x), cnn_result)\n",
        "\n",
        "    def fine_tune(self, need_fine_tune = True):\n",
        "        self.train(need_fine_tune)\n",
        "        self.backbone.eval()\n",
        "\n",
        "\n",
        "class Ensemble(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(Ensemble, self).__init__()\n",
        "        self.model = model\n",
        "        #freeze image backbone\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # self.image_encoder = nn.Sequential(\n",
        "        #     nn.Linear(2048, 1024),\n",
        "        #     nn.BatchNorm1d(1024),\n",
        "        #     nn.ReLU()\n",
        "        # )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(1024 + 2048 + 32, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, image, gender):\n",
        "        image_feature,  graph_feature, gender, result = self.model(image, gender)\n",
        "        # image_feature = self.image_encoder(image_feature)\n",
        "        if self.training:\n",
        "            return (self.fc(torch.cat([image_feature, graph_feature, gender], dim = 1)) + result[0])/2\n",
        "        else:\n",
        "            return (self.fc(torch.cat([image_feature, graph_feature, gender], dim = 1)) + result[0])/2\n",
        "\n",
        "\n",
        "    def fine_tune(self, need_fine_tune = True):\n",
        "        self.train(need_fine_tune)\n",
        "        self.model.eval()"
      ],
      "metadata": {
        "id": "cmcqEQ8cZ1sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training part!!**"
      ],
      "metadata": {
        "id": "dbgtXJ_2F7YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch import Tensor\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataloader import _utils\n",
        "\n",
        "from random import choice\n",
        "\n",
        "from skimage import io\n",
        "from PIL import Image, ImageOps\n",
        "\n",
        "import glob\n",
        "\n",
        "#from torchsummary import summary\n",
        "import logging\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "# from tqdm.notebook import tqdm\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "# from apex import amp\n",
        "\n",
        "import random\n",
        "\n",
        "import time\n",
        "\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from albumentations.augmentations.transforms import Lambda, ShiftScaleRotate, HorizontalFlip, Normalize, RandomBrightnessContrast, RandomResizedCrop\n",
        "from albumentations.pytorch import ToTensor\n",
        "from albumentations import Compose, OneOrOther\n",
        "\n",
        "import albumentations\n",
        "\n",
        "import warnings\n",
        "import torch_xla\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.data_parallel as dp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.test.test_utils as test_utils\n",
        "import warnings\n",
        "\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import time\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ysqXCM3PaMk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=1234):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    # torch.cuda.manual_seed(seed)\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "norm_mean = [0.143] #0.458971\n",
        "norm_std = [0.144] #0.225609\n",
        "\n",
        "RandomErasing = transforms.RandomErasing(scale=(0.02, 0.08), ratio = (0.5, 2), p = 0.8)\n",
        "\n",
        "def randomErase(image, **kwargs):\n",
        "    return RandomErasing(image)\n",
        "\n",
        "def sample_normalize(image, **kwargs):\n",
        "    image = image/255\n",
        "    channel = image.shape[2]\n",
        "    mean, std = image.reshape((-1, channel)).mean(axis = 0), image.reshape((-1, channel)).std(axis = 0)\n",
        "    return (image-mean)/(std + 1e-3)\n",
        "\n",
        "transform_train = Compose([\n",
        "    # RandomBrightnessContrast(p = 0.8),\n",
        "    RandomResizedCrop(512, 512, (0.5, 1.0), p = 0.5),\n",
        "    ShiftScaleRotate(shift_limit = 0.2, scale_limit = 0.2, rotate_limit=20, border_mode = cv2.BORDER_CONSTANT, value = 0.0, p = 0.8),\n",
        "    # HorizontalFlip(p = 0.5),\n",
        "\n",
        "    # ShiftScaleRotate(shift_limit = 0.2, scale_limit = 0.2, rotate_limit=20, p = 0.8),\n",
        "    HorizontalFlip(p = 0.5),\n",
        "    RandomBrightnessContrast(p = 0.8, contrast_limit=(-0.3, 0.2)),\n",
        "    Lambda(image = sample_normalize),\n",
        "    ToTensor(),\n",
        "    Lambda(image = randomErase)\n",
        "\n",
        "])\n",
        "\n",
        "transform_val = Compose([\n",
        "    Lambda(image = sample_normalize),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = Compose([\n",
        "    Lambda(image = sample_normalize),\n",
        "    ToTensor(),\n",
        "])\n",
        "\n",
        "\n",
        "def read_image(path, image_size = 512):\n",
        "    img = Image.open(path)\n",
        "    w, h = img.size\n",
        "    long = max(w, h)\n",
        "    w, h = int(w/long*image_size), int(h/long*image_size)\n",
        "    img = img.resize((w, h), Image.ANTIALIAS)\n",
        "    delta_w, delta_h = image_size - w, image_size - h\n",
        "    padding = (delta_w//2, delta_h//2, delta_w-(delta_w//2), delta_h-(delta_h//2))\n",
        "    return np.array(ImageOps.expand(img, padding).convert(\"RGB\"))\n",
        "\n",
        "\n",
        "class BAATrainDataset(Dataset):\n",
        "    def __init__(self, df, file_path):\n",
        "        def preprocess_df(df):\n",
        "            #nomalize boneage distribution\n",
        "            df['zscore'] = df['boneage'].map(lambda x: (x - boneage_mean)/boneage_div )\n",
        "            #change the type of gender, change bool variable to float32\n",
        "            df['male'] = df['male'].astype('float32')\n",
        "            df['bonage'] = df['boneage'].astype('float32')\n",
        "            return df\n",
        "\n",
        "        self.df = preprocess_df(df)\n",
        "        self.file_path = file_path\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        num = int(row['id'])\n",
        "        return (transform_train(image = read_image(f\"{self.file_path}/{num}.png\"))['image'], Tensor([row['male']])), row['zscore']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "class BAAValDataset(Dataset):\n",
        "    def __init__(self, df, file_path):\n",
        "        def preprocess_df(df):\n",
        "            #change the type of gender, change bool variable to float32\n",
        "            df['male'] = df['male'].astype('float32')\n",
        "            df['bonage'] = df['boneage'].astype('float32')\n",
        "            return df\n",
        "        self.df = preprocess_df(df)\n",
        "        self.file_path = file_path\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        return (transform_val(image = read_image(f\"{self.file_path}/{int(row['id'])}.png\"))['image'], Tensor([row['male']])), row['boneage']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "class BAATestDataset(Dataset):\n",
        "    def __init__(self, df, file_path):\n",
        "        def preprocess_df(df):\n",
        "            #change the type of gender, change bool variable to float32\n",
        "            df['male'] = (df['Sex'] == 'M').astype('float32')\n",
        "            df['boneage'] = df['Ground truth bone age (months)'].astype('float32')\n",
        "            df['id'] = df['Case ID'].astype('int32')\n",
        "            return df\n",
        "        self.df = preprocess_df(df)\n",
        "        print(self.df.head())\n",
        "        self.file_path = file_path\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        return (transform_test(image = read_image(f\"{self.file_path}/{int(row['id'])}.png\"))['image'], Tensor([row['male']])), row['boneage']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "def create_data_loader(train_df, val_df, test_df, train_root, val_root, test_root):\n",
        "    return BAATrainDataset(train_df, train_root), BAAValDataset(val_df, val_root), BAATestDataset(test_df, test_root)\n",
        "\n",
        "\n",
        "def L1_penalty(net, alpha):\n",
        "    l1_penalty = torch.nn.L1Loss(size_average = False)\n",
        "    loss = 0\n",
        "    for param in net.fc.parameters():\n",
        "        loss += torch.sum(torch.abs(param))\n",
        "\n",
        "    return alpha*loss\n",
        "\n",
        "\n",
        "def train_fn(net, train_loader, loss_fn, epoch, optimizer, device):\n",
        "    '''\n",
        "    checkpoint is a dict\n",
        "    '''\n",
        "    global total_size\n",
        "    global training_loss\n",
        "\n",
        "    net.fine_tune()\n",
        "    if xm.is_master_ordinal():\n",
        "        train_pbar = tqdm(train_loader)\n",
        "        train_pbar.desc = f'Epoch {epoch + 1}'\n",
        "    else:\n",
        "        train_pbar = train_loader\n",
        "    for batch_idx, data in enumerate(train_pbar):\n",
        "        # #put data to GPU\n",
        "        size = len(data[1])\n",
        "\n",
        "        image, gender = data[0]\n",
        "        image, gender= image.to(device), gender.to(device)\n",
        "\n",
        "        label = data[1].to(device)\n",
        "\n",
        "        batch_size = len(data[1])\n",
        "        label = data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        #forward\n",
        "        y_pred = net(image, gender)\n",
        "        y_pred = y_pred.squeeze()\n",
        "\n",
        "        # print(y_pred, label)\n",
        "        loss = loss_fn(y_pred, label)\n",
        "        #backward,calculate gradients\n",
        "        total_loss = loss + L1_penalty(net, 1e-5)\n",
        "        total_loss.backward()\n",
        "        #backward,update parameter\n",
        "        xm.optimizer_step(optimizer)\n",
        "\n",
        "        #the learning rate should be update after optimizer's update\n",
        "        #change the learning rate, because using One cycle pollicy,the learning rate should be update per mini-batch\n",
        "        # scheduler.step()\n",
        "\n",
        "        batch_loss = loss.item()\n",
        "\n",
        "        training_loss += batch_loss\n",
        "        total_size += batch_size\n",
        "        if xm.is_master_ordinal():\n",
        "            train_pbar.set_postfix({'loss': batch_loss/batch_size})\n",
        "        # print('loss:', batch_loss/batch_size)\n",
        "        # print(f'xla:{xm.get_ordinal()}, batch is{batch_idx}, loss is {mse_loss/total_size}, {size}')\n",
        "    return training_loss/total_size\n",
        "\n",
        "def evaluate_fn(net, val_loader, device):\n",
        "    net.fine_tune(False)\n",
        "\n",
        "    global mae_loss\n",
        "    global val_total_size\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(val_loader):\n",
        "            val_total_size += len(data[1])\n",
        "\n",
        "            image, gender = data[0]\n",
        "            image, gender= image.to(device), gender.to(device)\n",
        "\n",
        "            label = data[1].to(device)\n",
        "\n",
        "            y_pred = net(image, gender)*boneage_div+boneage_mean\n",
        "            # y_pred = net(image, gender)\n",
        "            y_pred = y_pred.squeeze()\n",
        "\n",
        "            batch_loss = F.l1_loss(y_pred, label, reduction='sum').item()\n",
        "            # print(batch_loss/len(data[1]))\n",
        "            mae_loss += batch_loss\n",
        "    return mae_loss\n",
        "\n",
        "\n",
        "def test_fn(net, test_loader, device):\n",
        "    net.train(False)\n",
        "\n",
        "    global test_mae_loss\n",
        "    global test_total_size\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader):\n",
        "            test_total_size += len(data[1])\n",
        "\n",
        "            image, gender = data[0]\n",
        "            image, gender= image.to(device), gender.to(device)\n",
        "\n",
        "            label = data[1].to(device)\n",
        "\n",
        "            y_pred = net(image, gender)*boneage_div+boneage_mean\n",
        "            # y_pred = net(image, gender)\n",
        "            y_pred = y_pred.squeeze()\n",
        "\n",
        "            batch_loss = F.l1_loss(y_pred, label, reduction='sum').item()\n",
        "            # print(batch_loss/len(data[1]))\n",
        "            test_mae_loss += batch_loss\n",
        "    return mae_loss\n",
        "\n",
        "\n",
        "def reduce_fn(vals):\n",
        "    return sum(vals)\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "def map_fn(index, flags):\n",
        "\n",
        "  ## Setup\n",
        "  root = '/content/drive/MyDrive/data/boneage-validation-dataset-1'\n",
        "  model_name = 'rsa50_4.48'\n",
        "  path = f'{root}/{model_name}'\n",
        "\n",
        "  if xm.is_master_ordinal():\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n",
        "\n",
        "  # Sets a common random seed - both for initialization and ensuring graph is the same\n",
        "  seed_everything(seed=flags['seed'])\n",
        "\n",
        "  # Acquires the (unique) Cloud TPU core corresponding to this process's index\n",
        "  device = xm.xla_device()\n",
        "\n",
        "\n",
        "#   mymodel = BAA_base(32)\n",
        "  mymodel = BAA_New(32, *get_My_resnet50())\n",
        "#   mymodel.load_state_dict(torch.load('/content/drive/My Drive/BAA/resnet50_pr_2/best_resnet50_pr_2.bin'))\n",
        "  mymodel = mymodel.to(device)\n",
        "\n",
        "  # Creates the (distributed) train sampler, which let this process only access\n",
        "  # its portion of the training dataset.\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_set,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "\n",
        "  val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    val_set,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)\n",
        "\n",
        "  test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    test_set,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)\n",
        "\n",
        "  # Creates dataloaders, which load data in batches\n",
        "  # Note: test loader is not shuffled or sampled\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_set,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=train_sampler,\n",
        "      num_workers=flags['num_workers'],\n",
        "      drop_last=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "      val_set,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=val_sampler,\n",
        "      shuffle=False,\n",
        "      num_workers=flags['num_workers'])\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_set,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=test_sampler,\n",
        "      shuffle=False,\n",
        "      num_workers=flags['num_workers'])\n",
        "\n",
        "  ## Network, optimizer, and loss function creation\n",
        "\n",
        "  # Creates AlexNet for 10 classes\n",
        "  # Note: each process has its own identical copy of the model\n",
        "  #  Even though each model is created independently, they're also\n",
        "  #  created in the same way.\n",
        "  net = mymodel.train()\n",
        "\n",
        "  global best_loss\n",
        "  best_loss = float('inf')\n",
        "#   loss_fn =  nn.MSELoss(reduction = 'sum')\n",
        "  loss_fn = nn.L1Loss(reduction = 'sum')\n",
        "  lr = flags['lr']\n",
        "\n",
        "  wd = 0\n",
        "\n",
        "  optimizer = torch.optim.Adam(mymodel.parameters(), lr=lr, weight_decay=wd)\n",
        "#   optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay = wd)\n",
        "  scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "  ## Trains\n",
        "  train_start = time.time()\n",
        "  for epoch in range(flags['num_epochs']):\n",
        "    global training_loss\n",
        "    training_loss = torch.tensor([0], dtype = torch.float32)\n",
        "    global total_size\n",
        "    total_size = torch.tensor([0], dtype = torch.float32)\n",
        "\n",
        "    global mae_loss\n",
        "    mae_loss = torch.tensor([0], dtype = torch.float32)\n",
        "    global val_total_size\n",
        "    val_total_size = torch.tensor([0], dtype = torch.float32)\n",
        "\n",
        "    global test_mae_loss\n",
        "    test_mae_loss = torch.tensor([0], dtype = torch.float32)\n",
        "    global test_total_size\n",
        "    test_total_size = torch.tensor([0], dtype = torch.float32)\n",
        "    # xm.rendezvous(\"initialization\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "    train_fn(net, para_train_loader, loss_fn, epoch, optimizer, device)\n",
        "\n",
        "    ## Evaluation\n",
        "    # Sets net to eval and no grad context\n",
        "    para_val_loader = pl.ParallelLoader(val_loader, [device]).per_device_loader(device)\n",
        "    evaluate_fn(net, para_val_loader, device)\n",
        "\n",
        "    para_test_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
        "    test_fn(net, para_test_loader, device)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    xm.save(net.state_dict(), '/'.join([path, f'{model_name}.bin']))\n",
        "    training_loss = xm.mesh_reduce('training_loss',training_loss,reduce_fn)\n",
        "    total_size = xm.mesh_reduce('total_size_reduce',total_size,reduce_fn)\n",
        "    mae_loss = xm.mesh_reduce('mae_loss_reduce',mae_loss,reduce_fn)\n",
        "    val_total_size = xm.mesh_reduce('val_total_size_reduce',val_total_size,reduce_fn)\n",
        "    test_mae_loss = xm.mesh_reduce('test_mae_loss_reduce',test_mae_loss,reduce_fn)\n",
        "    test_total_size = xm.mesh_reduce('test_total_size_reduce',test_total_size,reduce_fn)\n",
        "\n",
        "    if xm.is_master_ordinal():\n",
        "        print(test_total_size)\n",
        "        train_loss, val_mae, test_mae = training_loss/total_size, mae_loss/val_total_size, test_mae_loss/test_total_size\n",
        "        print(f'training loss is {train_loss}, val loss is {val_mae}, test loss is {test_mae}, time : {time.time() - start_time}, lr:{optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "\n",
        "    if xm.is_master_ordinal() and best_loss >= test_mae:\n",
        "        best_loss = test_mae\n",
        "        shutil.copy(f'{path}/{model_name}.bin', \\\n",
        "                    f'{path}/best_{model_name}.bin')\n",
        "\n",
        "\n",
        "\n",
        "def map_ensemble_fn(index, flags):\n",
        "\n",
        "  ## Setup\n",
        "  root = '/content/drive/MyDrive/data/boneage-validation-dataset-1'\n",
        "  model_name = 'final_ensemble_3.88'\n",
        "  path = f'{root}/{model_name}'\n",
        "\n",
        "  if xm.is_master_ordinal():\n",
        "    if not os.path.exists(path):\n",
        "        os.mkdir(path)\n",
        "\n",
        "  # Sets a common random seed - both for initialization and ensuring graph is the same\n",
        "  seed_everything(seed=flags['seed'])\n",
        "\n",
        "  # Acquires the (unique) Cloud TPU core corresponding to this process's index\n",
        "  device = xm.xla_device()\n",
        "\n",
        "\n",
        "#   mymodel = BAA_base(32)\n",
        "  net = Ensemble(new_model)\n",
        "#   mymodel.load_state_dict(torch.load('/content/drive/My Drive/BAA/resnet50_pr_2/best_resnet50_pr_2.bin'))\n",
        "  net = net.to(device)\n",
        "\n",
        "  # Creates the (distributed) train sampler, which let this process only access\n",
        "  # its portion of the training dataset.\n",
        "  train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_set,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "\n",
        "  val_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    val_set,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)\n",
        "\n",
        "  test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    test_set,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)\n",
        "\n",
        "  # Creates dataloaders, which load data in batches\n",
        "  # Note: test loader is not shuffled or sampled\n",
        "  train_loader = torch.utils.data.DataLoader(\n",
        "      train_set,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=train_sampler,\n",
        "      num_workers=flags['num_workers'],\n",
        "      drop_last=True)\n",
        "\n",
        "  val_loader = torch.utils.data.DataLoader(\n",
        "      val_set,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=val_sampler,\n",
        "      shuffle=False,\n",
        "      num_workers=flags['num_workers'])\n",
        "\n",
        "  test_loader = torch.utils.data.DataLoader(\n",
        "      test_set,\n",
        "      batch_size=flags['batch_size'],\n",
        "      sampler=test_sampler,\n",
        "      shuffle=False,\n",
        "      num_workers=flags['num_workers'])\n",
        "\n",
        "  ## Network, optimizer, and loss function creation\n",
        "\n",
        "  # Creates AlexNet for 10 classes\n",
        "  # Note: each process has its own identical copy of the model\n",
        "  #  Even though each model is created independently, they're also\n",
        "  #  created in the same way.\n",
        "  net.fine_tune()\n",
        "\n",
        "  global best_loss\n",
        "  best_loss = float('inf')\n",
        "#   loss_fn =  nn.MSELoss(reduction = 'sum')\n",
        "  loss_fn = nn.L1Loss(reduction = 'sum')\n",
        "  lr = flags['lr']\n",
        "\n",
        "  wd = 0\n",
        "\n",
        "  optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=lr, weight_decay=wd)\n",
        "#   optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay = wd)\n",
        "  scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "  ## Trains\n",
        "  train_start = time.time()\n",
        "  for epoch in range(flags['num_epochs']):\n",
        "    global training_loss\n",
        "    training_loss = torch.tensor([0], dtype = torch.float32)\n",
        "    global total_size\n",
        "    total_size = torch.tensor([0], dtype = torch.float32)\n",
        "\n",
        "    global mae_loss\n",
        "    mae_loss = torch.tensor([0], dtype = torch.float32)\n",
        "    global val_total_size\n",
        "    val_total_size = torch.tensor([0], dtype = torch.float32)\n",
        "\n",
        "    global test_mae_loss\n",
        "    test_mae_loss = torch.tensor([0], dtype = torch.float32)\n",
        "    global test_total_size\n",
        "    test_total_size = torch.tensor([0], dtype = torch.float32)\n",
        "    # xm.rendezvous(\"initialization\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "    train_fn(net, para_train_loader, loss_fn, epoch, optimizer, device)\n",
        "\n",
        "    ## Evaluation\n",
        "    # Sets net to eval and no grad context\n",
        "    para_val_loader = pl.ParallelLoader(val_loader, [device]).per_device_loader(device)\n",
        "    evaluate_fn(net, para_val_loader, device)\n",
        "\n",
        "    para_test_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
        "    test_fn(net, para_test_loader, device)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    xm.save(net.state_dict(), '/'.join([path, f'{model_name}.bin']))\n",
        "    training_loss = xm.mesh_reduce('training_loss',training_loss,reduce_fn)\n",
        "    total_size = xm.mesh_reduce('total_size_reduce',total_size,reduce_fn)\n",
        "    mae_loss = xm.mesh_reduce('mae_loss_reduce',mae_loss,reduce_fn)\n",
        "    val_total_size = xm.mesh_reduce('val_total_size_reduce',val_total_size,reduce_fn)\n",
        "    test_mae_loss = xm.mesh_reduce('test_mae_loss_reduce',test_mae_loss,reduce_fn)\n",
        "    test_total_size = xm.mesh_reduce('test_total_size_reduce',test_total_size,reduce_fn)\n",
        "\n",
        "    if xm.is_master_ordinal():\n",
        "        print(test_total_size)\n",
        "        train_loss, val_mae, test_mae = training_loss/total_size, mae_loss/val_total_size, test_mae_loss/test_total_size\n",
        "        print(f'training loss is {train_loss}, val loss is {val_mae}, test loss is {test_mae}, time : {time.time() - start_time}, lr:{optimizer.param_groups[0][\"lr\"]}')\n",
        "\n",
        "\n",
        "    if xm.is_master_ordinal() and best_loss >= test_mae:\n",
        "        best_loss = test_mae\n",
        "        shutil.copy(f'{path}/{model_name}.bin', \\\n",
        "                    f'{path}/best_{model_name}.bin')\n",
        "\n"
      ],
      "metadata": {
        "id": "QQO2kTD1GIZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training on a very small scale"
      ],
      "metadata": {
        "id": "t91UmKFnNN3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Set paths\n",
        "DATA_DIR = \"/content/drive/MyDrive/data/boneage-validation-dataset-1\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/data/Validation.csv\"\n",
        "\n",
        "\n",
        "# ✅ Load dataset and normalize stats\n",
        "train_df = pd.read_csv(CSV_PATH)\n",
        "train_df = train_df.rename(columns={\"Bone Age (months)\": \"boneage\"})\n",
        "train_df.rename(columns={'Image ID': 'id', 'Bone Age (months)': 'boneage'}, inplace=True)\n",
        "\n",
        "boneage_mean = train_df['boneage'].mean()\n",
        "boneage_div = train_df['boneage'].std()\n",
        "\n",
        "# ✅ Define transforms\n",
        "transform_train = Compose([\n",
        "    RandomResizedCrop(512, 512, (0.5, 1.0), p=0.5),\n",
        "    ShiftScaleRotate(shift_limit=0.2, scale_limit=0.2, rotate_limit=20, border_mode=0, value=0.0, p=0.8),\n",
        "    HorizontalFlip(p=0.5),\n",
        "    RandomBrightnessContrast(p=0.8, contrast_limit=(-0.3, 0.2)),\n",
        "    Lambda(image=sample_normalize),\n",
        "    ToTensor(),\n",
        "    Lambda(image=randomErase)\n",
        "])\n",
        "\n",
        "\n",
        "# ✅ TPU train-only loop\n",
        "def train_single(index, flags):\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    device = xm.xla_device()\n",
        "    xm.master_print(f\"Using device: {device}\")\n",
        "\n",
        "    model = BAA_New(32, *get_My_resnet50()).to(device)\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=flags['lr'])\n",
        "    loss_fn = nn.L1Loss()\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "        train_set,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=True\n",
        "    )\n",
        "    train_loader = DataLoader(train_set, batch_size=flags['batch_size'],\n",
        "                              sampler=train_sampler, num_workers=2, drop_last=True)\n",
        "    para_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "\n",
        "    for epoch in range(flags['epochs']):\n",
        "        model.train()\n",
        "        total_loss, total_samples = 0, 0\n",
        "        for (img, gender), label in para_loader:\n",
        "            img, gender, label = img.to(device), gender.to(device), label.to(device)\n",
        "            output = model(img, gender)[-1].squeeze()\n",
        "            loss = loss_fn(output * boneage_div + boneage_mean, label * boneage_div + boneage_mean)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            total_samples += len(label)\n",
        "\n",
        "        xm.master_print(f\"Epoch {epoch+1} | MAE: {total_loss/total_samples:.4f}\")\n",
        "\n",
        "# ✅ Launch training\n",
        "flags = {\n",
        "    \"lr\": 1e-4,\n",
        "    \"batch_size\": 8,\n",
        "    \"epochs\": 3\n",
        "}\n",
        "xmp.spawn(train_single, args=(flags,),start_method='fork')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dNfvdlJoNQt2",
        "outputId": "67fed188-4306-4e13-eca6-6cb6820f782a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: xla:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
            "Exception in thread Thread-13 (_loader_worker):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/parallel_loader.py\", line 165, in _loader_worker\n",
            "    _, data = next(data_iter)\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "    data = self._next_data()\n",
            "          WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            " WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
            "^^^^^^^^^^^^^^^^^\n",
            "Exception in thread   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            "Thread-13 (_loader_worker)    :\n",
            "Traceback (most recent call last):\n",
            "return self._process_data(data)  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "\n",
            "     self.run()\n",
            "    File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            "      WARNING:root:torch_xla.core.xla_model.xrt_world_size() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.world_size instead.\n",
            "WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
            "self._target(*self._args, **self._kwargs)WARNING:root:torch_xla.core.xla_model.xla_model.get_ordinal() will be removed in release 2.7. is deprecated. Use torch_xla.runtime.global_ordinal instead.\n",
            "  \n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/parallel_loader.py\", line 165, in _loader_worker\n",
            "      _, data = next(data_iter) \n",
            " ^^ ^ ^ ^^ ^ ^^ ^ ^ ^ ^ ^ ^   ^^^Exception in thread Thread-13 (_loader_worker):\n",
            "Traceback (most recent call last):\n",
            "^Exception in thread ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            "Exception in thread Thread-15 (_loader_worker)    :\n",
            "^Thread-13 (_loader_worker)^^:\n",
            "Traceback (most recent call last):\n",
            "data.reraise()^^Exception in thread ^  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "\n",
            "Thread-15 (_loader_worker)^Exception in thread :\n",
            "Traceback (most recent call last):\n",
            "^      File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n",
            "Traceback (most recent call last):\n",
            "^  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "Thread-15 (_loader_worker)self.run()  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "^    \n",
            ":\n",
            "raise exception    ^      File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "\n",
            "Traceback (most recent call last):\n",
            "self.run()^self.run()    \n",
            "FileNotFoundErrorself._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            ":   File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "Caught FileNotFoundError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"<ipython-input-4-2d62e1b5370e>\", line 77, in __getitem__\n",
            "    return (transform_train(image = read_image(f\"{self.file_path}/{num//1000}/{num}.png\"))['image'], Tensor([row['male']])), row['zscore']\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-cb8ca6f7be9f>\", line 51, in read_image\n",
            "    img = Image.open(path)\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3505, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/data/boneage-validation-dataset-1/2/2548.png'\n",
            "      File \"/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/parallel_loader.py\", line 165, in _loader_worker\n",
            "    \n",
            "        self.run()    data = self._next_data()_, data = next(data_iter)\n",
            "\n",
            "self._target(*self._args, **self._kwargs)self._target(*self._args, **self._kwargs)\n",
            "    File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "\n",
            " \n",
            "        File \"/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/parallel_loader.py\", line 165, in _loader_worker\n",
            "       File \"/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/parallel_loader.py\", line 165, in _loader_worker\n",
            "self.run() self._target(*self._args, **self._kwargs)     \n",
            "      _, data = next(data_iter)\n",
            "  _, data = next(data_iter)\n",
            "  \n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/parallel_loader.py\", line 165, in _loader_worker\n",
            "         Exception in thread   File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "     Thread-15 (_loader_worker) _, data = next(data_iter)   ^      :\n",
            "\n",
            " ^ ^  Traceback (most recent call last):\n",
            "self._target(*self._args, **self._kwargs) ^  ^ \n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n",
            "  ^^  File \"/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/parallel_loader.py\", line 165, in _loader_worker\n",
            " ^      ^ ^  ^      self.run()^^   ^^_, data = next(data_iter) \n",
            "  ^^ ^\n",
            "   File \"/usr/lib/python3.11/threading.py\", line 982, in run\n",
            "  ^^   ^  ^       \n",
            " ^   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "self._target(*self._args, **self._kwargs) ^^^     ^^^data = self._next_data() ^\n",
            "^\n",
            "^^^ ^   File \"/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/parallel_loader.py\", line 165, in _loader_worker\n",
            " ^^^  ^^^ ^^      ^^^ ^_, data = next(data_iter)^^  ^ ^^^^ \n",
            "^  ^ ^ ^ ^^^^^^  ^^^^^ ^^ ^\n",
            "^^^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "  ^^^    \n",
            "^ ^ data = self._next_data()^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            "\n",
            " \n",
            "^^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            " ^ ^^      ^^    ^data = self._next_data()  ^^\n",
            "return self._process_data(data)\n",
            " ^ ^ ^  \n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            " ^   ^^      \n",
            "  ^    File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            " data = self._next_data()  ^     ^   return self._process_data(data)\n",
            "^^\n",
            "  ^ ^  ^ ^ ^^ ^   ^^   ^ ^ ^^    ^ ^\n",
            "^  ^ ^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "^   ^ ^    ^ ^data = self._next_data()  ^^^^\n",
            "^^ ^^  ^^^^^ ^^ ^^ ^^^^  ^\n",
            "^^^ ^^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
            "^^^^ ^^^^^    ^^^ ^data = self._next_data()^^ ^\n",
            "^^^^^^^^  ^^^^^^^ ^ \n",
            "^^^^^   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            " ^^^ ^^^^    ^^\n",
            "^return self._process_data(data) ^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            "\n",
            " ^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            "      ^ data.reraise()^^\n",
            "     ^ return self._process_data(data)^  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n",
            "^ ^ ^\n",
            "    ^^^   ^raise exception^^  ^\n",
            "^^ ^ ^FileNotFoundError^^ ^^ : ^\n",
            "^^  Caught FileNotFoundError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"<ipython-input-4-2d62e1b5370e>\", line 77, in __getitem__\n",
            "    return (transform_train(image = read_image(f\"{self.file_path}/{num//1000}/{num}.png\"))['image'], Tensor([row['male']])), row['zscore']\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-cb8ca6f7be9f>\", line 51, in read_image\n",
            "    img = Image.open(path)\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3505, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/data/boneage-validation-dataset-1/10/10958.png'\n",
            "^   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            "^^\n",
            "  ^ ^^^^^^^     ^^\n",
            "^ ^^return self._process_data(data)  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            "^ \n",
            "^^\n",
            "     ^^   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            " data.reraise()^^ ^    ^^\n",
            " ^return self._process_data(data)^  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n",
            " ^\n",
            "^^     ^ ^raise exception^ ^^ ^\n",
            " ^^ ^FileNotFoundError^^ ^ \n",
            ":  ^^   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1480, in _next_data\n",
            "Caught FileNotFoundError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"<ipython-input-4-2d62e1b5370e>\", line 77, in __getitem__\n",
            "    return (transform_train(image = read_image(f\"{self.file_path}/{num//1000}/{num}.png\"))['image'], Tensor([row['male']])), row['zscore']\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-cb8ca6f7be9f>\", line 51, in read_image\n",
            "    img = Image.open(path)\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3505, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/data/boneage-validation-dataset-1/2/2542.png'\n",
            "^ ^  ^\n",
            "^      ^^^return self._process_data(data) ^^^\n",
            " ^^^  ^^^ ^^^^ ^^^^ ^^^^ ^^^^^ ^^^ ^^^^\n",
            "^ ^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            "^ ^^ ^^    ^ ^^data.reraise()^^\n",
            "^\n",
            "^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n",
            "^^^    ^^^    ^data.reraise()^^raise exception^^\n",
            "^\n",
            "^^  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n",
            "^FileNotFoundError^^    ^^: ^raise exception\n",
            "Caught FileNotFoundError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"<ipython-input-4-2d62e1b5370e>\", line 77, in __getitem__\n",
            "    return (transform_train(image = read_image(f\"{self.file_path}/{num//1000}/{num}.png\"))['image'], Tensor([row['male']])), row['zscore']\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-cb8ca6f7be9f>\", line 51, in read_image\n",
            "    img = Image.open(path)\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3505, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/data/boneage-validation-dataset-1/2/2334.png'\n",
            "^^^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            "\n",
            "\n",
            "^    FileNotFoundError^^: data.reraise()^^Caught FileNotFoundError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"<ipython-input-4-2d62e1b5370e>\", line 77, in __getitem__\n",
            "    return (transform_train(image = read_image(f\"{self.file_path}/{num//1000}/{num}.png\"))['image'], Tensor([row['male']])), row['zscore']\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-cb8ca6f7be9f>\", line 51, in read_image\n",
            "    img = Image.open(path)\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3505, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/data/boneage-validation-dataset-1/10/10445.png'\n",
            "\n",
            "^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n",
            "^^\n",
            "^      File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            "^raise exception^\n",
            "^    FileNotFoundError^data.reraise(): ^\n",
            "Caught FileNotFoundError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"<ipython-input-4-2d62e1b5370e>\", line 77, in __getitem__\n",
            "    return (transform_train(image = read_image(f\"{self.file_path}/{num//1000}/{num}.png\"))['image'], Tensor([row['male']])), row['zscore']\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-cb8ca6f7be9f>\", line 51, in read_image\n",
            "    img = Image.open(path)\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3505, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/data/boneage-validation-dataset-1/10/10686.png'\n",
            "^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n",
            "^    ^raise exception^\n",
            "\n",
            "FileNotFoundError  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1505, in _process_data\n",
            ":     Caught FileNotFoundError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"<ipython-input-4-2d62e1b5370e>\", line 77, in __getitem__\n",
            "    return (transform_train(image = read_image(f\"{self.file_path}/{num//1000}/{num}.png\"))['image'], Tensor([row['male']])), row['zscore']\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-cb8ca6f7be9f>\", line 51, in read_image\n",
            "    img = Image.open(path)\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3505, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/data/boneage-validation-dataset-1/9/9679.png'\n",
            "data.reraise()\n",
            "\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_utils.py\", line 733, in reraise\n",
            "    raise exception\n",
            "FileNotFoundError: Caught FileNotFoundError in DataLoader worker process 0.\n",
            "Original Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n",
            "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "            ~~~~~~~~~~~~^^^^^\n",
            "  File \"<ipython-input-4-2d62e1b5370e>\", line 77, in __getitem__\n",
            "    return (transform_train(image = read_image(f\"{self.file_path}/{num//1000}/{num}.png\"))['image'], Tensor([row['male']])), row['zscore']\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-11-cb8ca6f7be9f>\", line 51, in read_image\n",
            "    img = Image.open(path)\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3505, in open\n",
            "    fp = builtins.open(filename, \"rb\")\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/drive/MyDrive/data/boneage-validation-dataset-1/11/11270.png'\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 261, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 210, in _process_chunk\n    return [fn(*args) for args in chunk]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/concurrent/futures/process.py\", line 210, in <listcomp>\n    return [fn(*args) for args in chunk]\n            ^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch_xla/_internal/pjrt.py\", line 77, in _run_thread_per_device\n    replica_results = list(\n                      ^^^^^\n  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 619, in result_iterator\n    yield _result_or_cancel(fs.pop())\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 317, in _result_or_cancel\n    return fut.result(timeout)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/usr/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch_xla/_internal/pjrt.py\", line 70, in _thread_fn\n    return fn()\n           ^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch_xla/_internal/pjrt.py\", line 185, in __call__\n    self.fn(runtime.global_ordinal(), *self.args, **self.kwargs)\n  File \"<ipython-input-12-6a4a43426f9b>\", line 62, in train_single\n    xm.master_print(f\"Epoch {epoch+1} | MAE: {total_loss/total_samples:.4f}\")\n                                              ~~~~~~~~~~^~~~~~~~~~~~~~\nZeroDivisionError: division by zero\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6a4a43426f9b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;34m\"epochs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m }\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpjrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/_internal/pjrt.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, nprocs, start_method, args)\u001b[0m\n\u001b[1;32m    211\u001b[0m         % nprocs)\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m   \u001b[0mrun_multiprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspawn_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/_internal/pjrt.py\u001b[0m in \u001b[0;36mrun_multiprocess\u001b[0;34m(fn, start_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         initializer_fn=initialize_multiprocess)\n\u001b[1;32m    168\u001b[0m     \u001b[0mprocess_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     replica_results = list(\n\u001b[0m\u001b[1;32m    170\u001b[0m         itertools.chain.from_iterable(\n\u001b[1;32m    171\u001b[0m             result.items() for result in process_results))\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_xla/_internal/pjrt.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0mprocess_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     replica_results = list(\n\u001b[0;32m--> 170\u001b[0;31m         itertools.chain.from_iterable(\n\u001b[0m\u001b[1;32m    171\u001b[0m             result.items() for result in process_results))\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/process.py\u001b[0m in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0mcareful\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mto\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0mto\u001b[0m \u001b[0myielded\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \"\"\"\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0melement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    617\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/drive/MyDrive/data/saved_model.pth'\n",
        "xm.save(net.state_dict(), model_path)"
      ],
      "metadata": {
        "id": "903qv6ptW70z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Scale Implementation"
      ],
      "metadata": {
        "id": "XfVWhlI7WIn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "    #from model import Ensemble, Graph_BAA, BAA_New, get_My_resnet50, BAA_Base\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('model_type')\n",
        "    parser.add_argument('lr', type=float)\n",
        "    parser.add_argument('batch_size', type = int)\n",
        "    parser.add_argument('num_epochs', type = int)\n",
        "    parser.add_argument('seed', type = int)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.model_type == 'ensemble':\n",
        "        model = BAA_New(32, *get_My_resnet50())\n",
        "        model.load_state_dict(torch.load('/content/drive/MyDrive/data/boneage-validation-dataset-1/MRSA_50++_4.03/best_MRSA_50++_4.03.bin'))\n",
        "        new_model = Graph_BAA(model)\n",
        "        ensemble = Ensemble(new_model)\n",
        "    else:\n",
        "        model = BAA_New(32, *get_My_resnet50())\n",
        "\n",
        "    flags = {}\n",
        "    flags['lr'] = args.lr\n",
        "    flags['batch_size'] = args.batch_size\n",
        "    flags['num_workers'] = 2\n",
        "    flags['num_epochs'] = args.num_epochs\n",
        "    flags['seed'] = args.seed\n",
        "\n",
        "    train_df = pd.read_csv(f'/content/drive/My Drive/BAA/train.csv')\n",
        "    val_df = pd.read_csv(f'/content/drive/My Drive/BAA/Validation Dataset.csv')\n",
        "    test_df = pd.read_excel('/content/drive/My Drive/BAA/Bone age ground truth.xlsx')\n",
        "    boneage_mean = train_df['boneage'].mean()\n",
        "    boneage_div = train_df['boneage'].std()\n",
        "    train_set, val_set, test_set = create_data_loader(train_df, val_df, test_df, '/content/drive/My Drive/BAA/boneage-training-dataset', '/content/drive/My Drive/BAA/boneage-validation-dataset', '/content/drive/My Drive/BAA/Test Set Images')\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    if args.model_type == 'ensemble':\n",
        "        xmp.spawn(map_ensemble_fn, args=(flags,), nprocs=8, start_method='fork')\n",
        "    else:\n",
        "        xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "metadata": {
        "id": "9kF1cJ_5ISNN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}